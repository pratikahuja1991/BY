{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9904b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Important libraries\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from scipy import stats\n",
    "from sklearn import preprocessing,metrics,linear_model\n",
    "from sklearn.model_selection import cross_val_score,cross_val_predict,train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import time\n",
    "\n",
    "#Read the training data\n",
    "bikehour = pd.read_csv('hour.csv')\n",
    "bikehour.head()\n",
    "bikeday = pd.read_csv('day.csv')\n",
    "bikeday.head()\n",
    "\n",
    "#Shape of the dataset\n",
    "bikeday.shape\n",
    "bikehour.shape\n",
    "\n",
    "#Data types\n",
    "bikeday.dtypes\n",
    "bikehour.dtypes\n",
    "\n",
    "#Rename the columns as its been mess in the dataset for names\n",
    "bikeday.rename(columns={'dteday':'datetime','yr':'year','mnth':'month','weathersit':'weather','hum':'humidity','cnt':'count'},inplace=True)\n",
    "bikehour.rename(columns={'dteday':'datetime','yr':'year','hr': 'hour','mnth':'month','weathersit':'weather','hum':'humidity','cnt':'count'},inplace=True)\n",
    "\n",
    "#Missing values in dataset\n",
    "bikeday.isnull().sum()\n",
    "bikehour.isnull().sum()\n",
    "\n",
    "#Type casting the datetime and numerical attributes to category and converting to categorical variable\n",
    "bikeday['datetime']=pd.to_datetime(bikeday.datetime)\n",
    "bikeday['season']=bikeday.season.astype('category')\n",
    "bikeday['year']=bikeday.year.astype('category')\n",
    "bikeday['month']=bikeday.month.astype('category')\n",
    "bikeday['holiday']=bikeday.holiday.astype('category')\n",
    "bikeday['weekday']=bikeday.weekday.astype('category')\n",
    "bikeday['workingday']=bikeday.workingday.astype('category')\n",
    "bikeday['weather']=bikeday.weather.astype('category')\n",
    "\n",
    "#Dropping of unwanted columns\n",
    "bikeday = bikeday.drop([\"instant\",\"datetime\"],axis = 1)\n",
    "\n",
    "#Summary of the dataset\n",
    "bikeday.describe()\n",
    "\n",
    "# Visualizing the usage of bike during weekday and weekends and the graph shows that the bikes are rented more\n",
    "# during weekdays than weekends. The bike are used mostly during office hours .\n",
    "fig, ax = plt.subplots(figsize=(20,10))\n",
    "sns.pointplot(data=bikehour[['hour','count','weekday']],x='hour', y='count',hue='weekday', ax=ax, aspect=.5)\n",
    "ax.set(title=\"Bike usage during weekdays and weekends\")\n",
    "\n",
    "# Visualizing the usage of bike during different weather conditions and the graph shows that the bikes are rented\n",
    "# more during weekdays than weekends. The use of bike declines once the weather condition becomes bad and there is\n",
    "# almost negligible data during heavy snow and thunderstrom. \n",
    "fig, ax = plt.subplots(figsize=(20,10))\n",
    "sns.pointplot(data=bikehour[['hour','count','weather']],x='hour', y='count',hue='weather', ax=ax, aspect=.8)\n",
    "ax.set(title=\"Bike usage during different weather condition\")\n",
    "\n",
    "fig,ax=plt.subplots(figsize=(15,8))\n",
    "sns.set_style('white')\n",
    "#Bar plot for seasonwise monthly distribution of counts\n",
    "sns.barplot(x='month',y='count',data=bikeday[['month','count','season']],hue='season',ax=ax)\n",
    "ax.set_title('Seasonwise monthly distribution of counts')\n",
    "plt.show()\n",
    "#Bar plot for weekday wise monthly distribution of counts\n",
    "fig,ax1=plt.subplots(figsize=(15,8))\n",
    "sns.barplot(x='month',y='count',data=bikeday[['month','count','weekday']],hue='weekday',ax=ax1)\n",
    "ax1.set_title('Weekday wise monthly distribution of counts')\n",
    "plt.show()\n",
    "\n",
    "#Outliers Analysis\n",
    "fig, axes = plt.subplots(nrows=2,ncols=2)\n",
    "fig.set_size_inches(10,12)\n",
    "sns.boxplot(data=bikeday,y=\"count\",orient='v',ax=axes[0][0])\n",
    "sns.boxplot(data=bikeday,y=\"count\",x=\"season\",orient='v',ax=axes[0][1])\n",
    "sns.boxplot(data=bikeday,y=\"count\",x=\"weekday\",orient=\"v\",ax=axes[1][0])\n",
    "sns.boxplot(data=bikeday,y=\"count\",x=\"workingday\",orient=\"v\",ax=axes[1][1])\n",
    "\n",
    "axes[0][0].set(ylabel='count',title = \"Boxplot of count\")\n",
    "axes[0][1].set(xlabel=\"season\",ylabel=\"count\",title=\"Boxplot for count vs season\")\n",
    "axes[1][0].set(xlabel=\"weekday\", ylabel=\"count\",title=\"Boxplot for count vs weekday\")\n",
    "axes[1][1].set(xlabel=\"workingday\",ylabel=\"count\",title=\"Boxplot for count vs workingday\")\n",
    "\n",
    "\n",
    "#From the below boxplots, it is evident that there are no outliers present in the count. Two things are clear.\n",
    "#Count is very low in spring season.\n",
    "#Count is maximum when weather is good and its minimum weather is bad.\n",
    "\n",
    "fig.set_size_inches(8,12)\n",
    "sns.boxplot(data=bikeday, x=\"weather\",y=\"count\").set_title(\"Boxplot of count vs weather\")\n",
    "\n",
    "#From the box plot, we can observed that no outliers are present in count variable.\n",
    "fig,ax=plt.subplots(figsize=(15,8))\n",
    "#Boxplot for count outliers\n",
    "sns.boxplot(data=bikeday[['count']])\n",
    "ax.set_title('Total_count outliers')\n",
    "plt.show()\n",
    "\n",
    "#Create the correlation matrix\n",
    "correMtr=bikeday[[\"temp\",\"atemp\",\"humidity\",\"windspeed\",\"casual\",\"registered\",\"count\"]].corr()\n",
    "mask=np.array(correMtr)\n",
    "mask[np.tril_indices_from(mask)]=False\n",
    "\n",
    "#Heat map for correlation matrix of attributes\n",
    "fig,ax=plt.subplots(figsize=(15,8))\n",
    "sns.heatmap(correMtr,mask=mask,vmax=0.8,square=True,annot=True,ax=ax)\n",
    "ax.set_title('Correlation matrix of attributes')\n",
    "plt.show()\n",
    "\n",
    "#From correlation plot, we can observed that some features are positively correlated or some are negatively correlated to each other. \n",
    "#The temp and atemp are highly positively correlated to each other, it means that both are carrying same information.\n",
    "#The total_count,casual and registered are highly positively correlated to each other. \n",
    "#So, I will be going to ignore atemp,casual and registered variable for further analysis.\n",
    "\n",
    "# Bivariate analysis of count and continous predictor\n",
    "\n",
    "fig,(ax1,ax2,ax3,ax4) = plt.subplots(ncols=4)\n",
    "fig.set_size_inches(12,6)\n",
    "\n",
    "sns.regplot(x=\"temp\",y=\"count\",data=bikeday,ax=ax1)\n",
    "sns.regplot(x=\"atemp\",y=\"count\",data=bikeday,ax=ax2)\n",
    "sns.regplot(x=\"humidity\",y=\"count\",data=bikeday,ax=ax3)\n",
    "sns.regplot(x=\"windspeed\",y=\"count\",data=bikeday,ax=ax4)\n",
    "\n",
    "#From the above plot, it is evident that count has a positive linear relationship with temp and atemp. \n",
    "#On the other hand, count has a negative linear relationship with windspeed. \n",
    "#Humidity(hum) has a little negative linear relationship with count.\n",
    "\n",
    "#Distribution of target Variable\n",
    "\n",
    "fig,(ax1,ax2) = plt.subplots(ncols=2)\n",
    "fig.set_size_inches(9,5)\n",
    "sns.distplot(bikeday[\"count\"],ax=ax1)\n",
    "stats.probplot(bikeday[\"count\"], dist='norm', fit=True, plot=ax2)\n",
    "\n",
    "#As we can see, out cnt variable is very close to normal distribution.\n",
    "#Preprocessing original data and Spliting into train and test data.\n",
    "\n",
    "# selecting predictors\n",
    "train_feature_space = bikeday.iloc[:,bikeday.columns != 'count']\n",
    "# selecting target class\n",
    "target_class = bikeday.iloc[:,bikeday.columns == 'count']\n",
    "\n",
    "#droping atemp due to multicollinearity\n",
    "#droping casual and registered because there sum is equal to target variable ie. 'count'\n",
    "train_feature_space = train_feature_space.drop([\"atemp\",\"casual\",\"registered\"],axis = 1)\n",
    "train_feature_space.shape\n",
    "\n",
    "# creating training and test set\n",
    "training_set, test_set, train_taget, test_target = train_test_split(train_feature_space, target_class,test_size = 0.30,random_state = 456)\n",
    "\n",
    "# Cleaning test sets to avoid future warning messages\n",
    "train_taget = train_taget.values.ravel() \n",
    "test_target = test_target.values.ravel()\n",
    "\n",
    "# Model1 Linear Regression Model\n",
    "\n",
    "# Initialize logistic regression model\n",
    "lModel = LinearRegression()\n",
    "lModel.fit(X = training_set,y = np.log(train_taget))\n",
    "\n",
    "# Predicting using linear regression\n",
    "lmPredictions = lModel.predict(X=test_set)\n",
    "x=pd.DataFrame(np.exp(lmPredictions))\n",
    "x=pd.DataFrame(np.exp(lmPredictions))\n",
    "lm_errors = abs(np.exp(lmPredictions) - test_target)\n",
    "\n",
    "# Print out the mean absolute error (mae)\n",
    "print('Mean Absolute Error:', round(np.mean(lm_errors), 2), 'degrees.')\n",
    "#Print out root mean square error.\n",
    "rmse = sqrt(mean_squared_error(test_target, np.exp(lmPredictions)))\n",
    "print(\"RMSE for test set in linear regression is :\" , rmse)\n",
    "\n",
    "#Residual plot tells about finite variance between actual target value and predicted target value.\n",
    "#In this plot,very less data points are have same finite variance between them and for most are not have it.\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,8))\n",
    "ax.scatter(test_target,lm_errors)\n",
    "ax.axhline(lw=2,color='black')\n",
    "ax.set_xlabel('Observed')\n",
    "ax.set_ylabel('Residuals')\n",
    "ax.title.set_text(\"Residual Plot\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Model 2 Random Forest\n",
    "\n",
    "rf = RandomForestRegressor(random_state=12345)\n",
    "\n",
    "np.random.seed(12)\n",
    "start = time.time()\n",
    "\n",
    "# Selecting best max_depth, maximum features, split criterion and number of trees\n",
    "param_dist = {'max_depth': [2,4,6,8,10],\n",
    "              'bootstrap': [True, False],\n",
    "              'max_features': ['auto', 'sqrt', 'log2',None],\n",
    "              \"n_estimators\" : [100 ,200 ,300 ,400 ,500]\n",
    "             }\n",
    "cv_randomForest = RandomizedSearchCV(rf, cv = 10,\n",
    "                     param_distributions = param_dist, \n",
    "                     n_iter = 10)\n",
    "\n",
    "cv_randomForest.fit(training_set, train_taget)\n",
    "print('Best Parameters using random search: \\n', \n",
    "      cv_randomForest.best_params_)\n",
    "end = time.time()\n",
    "print('Time taken in random search: {0: .2f}'.format(end - start))\n",
    "\n",
    "\n",
    "# Setting parameters\n",
    "# Set best parameters given by random search\n",
    "\n",
    "rf.set_params( max_features = 'log2',max_depth =8 ,n_estimators = 300)\n",
    "\n",
    "# Fit the trained model\n",
    "rf.fit(training_set, train_taget)\n",
    "\n",
    "# Use the forest's predict method on the test data\n",
    "rfPredictions = rf.predict(test_set)\n",
    "# Calculate the absolute errors\n",
    "rf_errors = abs(rfPredictions - test_target)\n",
    "# Print out the mean absolute error (mae)\n",
    "print('Mean Absolute Error:', round(np.mean(rf_errors), 2), 'degrees.')\n",
    "\n",
    "# Print out root mean square error.\n",
    "rmse_rf = sqrt(mean_squared_error(test_target, rfPredictions))\n",
    "print(\"RMSE for test set in random forest regressor  is :\" , rmse_rf)\n",
    "\n",
    "#Residual scatter plot\n",
    "fig, ax = plt.subplots(figsize=(15,8))\n",
    "residuals=test_target-rf_errors\n",
    "ax.scatter(test_target, residuals)\n",
    "ax.axhline(lw=2,color='black')\n",
    "ax.set_xlabel('Observed')\n",
    "ax.set_ylabel('Residuals')\n",
    "ax.set_title('Residual plot')\n",
    "plt.show()\n",
    "\n",
    "#Cross validation prediction plot tells about finite variance between actual target value and predicted target value.\n",
    "#In this plot,some data points are have same finite variance between them and for some are not have it.\n",
    "\n",
    "# Variable importance for random forest\n",
    "feature_importance =  pd.Series(rf.feature_importances_, index=training_set.columns)\n",
    "feature_importance.plot(kind='barh')\n",
    "\n",
    "# Random forest accuracy score\n",
    "rf_score =rf.score(training_set,train_taget)\n",
    "print('Accuracy of the Random forest model :',rf_score)\n",
    "\n",
    "# Model 3 Decision tree regressor\n",
    "\n",
    "##training the model\n",
    "dtr=DecisionTreeRegressor(min_samples_split=2,max_leaf_nodes=10)\n",
    "\n",
    "#Fit the trained model\n",
    "dtr.fit(training_set,train_taget)\n",
    "dtr_pred=dtr.predict(test_set)\n",
    "\n",
    "# Print out the mean absolute error (mae)\n",
    "dtr_errors = abs(dtr_pred - test_target)\n",
    "print('Mean Absolute Error:', round(np.mean(dtr_errors), 2), 'degrees.')\n",
    "\n",
    "## Print out root mean square error.\n",
    "rmse_dtr = sqrt(mean_squared_error(test_target, dtr_pred))\n",
    "print(\"RMSE for test set in Decision Tree regressor  is :\" , rmse_dtr)\n",
    "\n",
    "# Residual plot tells about finite variance between actual target value and predicted target value. \n",
    "#In this plot, some data points are have same finite variance between them and for some are not have it.\n",
    "\n",
    "residuals = test_target-dtr_pred\n",
    "fig, ax = plt.subplots(figsize=(15,8))\n",
    "ax.scatter(test_target, residuals)\n",
    "ax.axhline(lw=2,color='black')\n",
    "ax.set_xlabel('Observed')\n",
    "ax.set_ylabel('Residual')\n",
    "ax.set_title('Residual plot')\n",
    "plt.show()\n",
    "\n",
    "#Decision tree regression accuracy score\n",
    "dtr_score =dtr.score(training_set,train_taget)\n",
    "print('Accuracy of the model :',dtr_score)\n",
    "\n",
    "# Final model for predicting the bike rental count on daily basis.\n",
    "# When we compare the root mean squared error and mean absolute error of all 3 models, the random forest model has less root mean squared error and mean absolute error. \n",
    "# So, finally I concluded Random forest model is bset for predicting the bike rental count on daily basis.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
